name: CI/CD for VLLM Deployment

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    # Checkout the repository
    - name: Checkout Code
      uses: actions/checkout@v3

    # Set up Docker
    - name: Set up Docker
      uses: docker/setup-buildx-action@v2
     # Install NVIDIA Container Toolkit
    - name: Install NVIDIA Container Toolkit
      run: |
        distribution=$(. /etc/os-release; echo $ID$VERSION_ID | sed -e 's/\.//g') \
        && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
        && curl -fsSL https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
           sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
           sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \
        && sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit \
        && sudo nvidia-ctk runtime configure --runtime=docker \
        && sudo systemctl restart docker


    # Log in to Docker Hub (optional if pulling from public Docker Hub)
    - name: Log in to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    # Pull the Docker image
    - name: Pull VLLM Docker Image
      run: |
        docker pull vllm/vllm-openai:latest

    # Run the Docker container
    - name: Run VLLM Container
      run: |
        docker run --runtime nvidia --gpus all \
          --name my_vllm_container \
          -v ~/.cache/huggingface:/root/.cache/huggingface \
          --env "HUGGING_FACE_HUB_TOKEN=${{ secrets.HUGGING_FACE_HUB_TOKEN }}" \
          -p 8000:8000 \
          --ipc=host \
          vllm/vllm-openai:latest \
          --model meta-llama/Llama-3.2-1B

    # Verify the container is running
    - name: Test Server Health
      run: |
        sleep 10 # Wait for the server to start
        curl -X POST "http://localhost:8000/v1/completions" \
          -H "Content-Type: application/json" \
          --data '{
            "model": "meta-llama/Llama-3.2-1B",
            "prompt": "Health Check:",
            "max_tokens": 5,
            "temperature": 0.1
          }'

    # Optional: Cleanup old containers
    - name: Cleanup old containers
      run: |
        docker container prune -f
