name: CI/CD for VLLM Deployment

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: self-hosted
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v3

    - name: Verify NVIDIA Drivers
      run: |
        nvidia-smi || echo "NVIDIA drivers not found"
        docker info | grep -i runtime

    - name: Set up Docker with NVIDIA Support
      run: |
        # Install NVIDIA Container Toolkit
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
        distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
          && curl -fsSL https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
          sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
        sudo apt-get update
        sudo apt-get install -y nvidia-container-toolkit
        
        # Configure Docker to use NVIDIA Container Runtime
        sudo nvidia-ctk runtime configure --runtime=docker
        
        # Create or update daemon.json
        echo '{
          "default-runtime": "nvidia",
          "runtimes": {
            "nvidia": {
              "path": "nvidia-container-runtime",
              "runtimeArgs": []
            }
          }
        }' | sudo tee /etc/docker/daemon.json
        
        # Restart Docker daemon
        sudo systemctl restart docker
        sleep 5

    - name: Verify NVIDIA Docker Configuration
      run: |
        docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
        
    - name: Log in to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Pull VLLM Docker Image
      run: docker pull vllm/vllm-openai:latest

    - name: Run VLLM Container
      run: |
        docker run -d --rm --gpus all \
          --name my_vllm_container \
          -v ~/.cache/huggingface:/root/.cache/huggingface \
          --env "HUGGING_FACE_HUB_TOKEN=${{ secrets.HUGGING_FACE_HUB_TOKEN }}" \
          -p 8000:8000 \
          --ipc=host \
          vllm/vllm-openai:latest \
          --model meta-llama/Llama-3.2-1B

    - name: Test Server Health
      run: |
        sleep 30
        curl -X POST "http://localhost:8000/v1/completions" \
          -H "Content-Type: application/json" \
          --data '{
            "model": "meta-llama/Llama-3.2-1B",
            "prompt": "Health Check:",
            "max_tokens": 5,
            "temperature": 0.1
          }'

    - name: Cleanup
      if: always()
      run: |
        docker stop my_vllm_container || true
        docker container prune -f
