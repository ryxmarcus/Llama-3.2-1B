name: CI/CD for VLLM Deployment

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    # Checkout the repository
    - name: Checkout Code
      uses: actions/checkout@v3

    # Set up Docker
    - name: Set up Docker
      uses: docker/setup-buildx-action@v2

    # Log in to Docker Hub (optional if pulling from public Docker Hub)
    - name: Log in to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    # Pull the Docker image
    - name: Pull VLLM Docker Image
      run: |
        docker pull vllm/vllm-openai:latest

    # Run the Docker container
    - name: Run VLLM Container
      run: |
        docker run --runtime nvidia --gpus all \
          --name my_vllm_container \
          -v ~/.cache/huggingface:/root/.cache/huggingface \
          --env "HUGGING_FACE_HUB_TOKEN=${{ secrets.HUGGING_FACE_HUB_TOKEN }}" \
          -p 8000:8000 \
          --ipc=host \
          vllm/vllm-openai:latest \
          --model meta-llama/Llama-3.2-1B

    # Verify the container is running
    - name: Test Server Health
      run: |
        sleep 10 # Wait for the server to start
        curl -X POST "http://localhost:8000/v1/completions" \
          -H "Content-Type: application/json" \
          --data '{
            "model": "meta-llama/Llama-3.2-1B",
            "prompt": "Health Check:",
            "max_tokens": 5,
            "temperature": 0.1
          }'

    # Optional: Cleanup old containers
    - name: Cleanup old containers
      run: |
        docker container prune -f
