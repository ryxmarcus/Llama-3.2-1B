name: CI/CD for VLLM Deployment
on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    # Change runner to one with GPU support
    runs-on: ubuntu-latest
    container:
      image: nvidia/cuda:11.8.0-base-ubuntu22.04
      options: --gpus all

    steps:
    - name: Checkout Code
      uses: actions/checkout@v3

    # Install basic dependencies
    - name: Install Dependencies
      run: |
        apt-get update && apt-get install -y curl gnupg2 software-properties-common
        
    # Set up Docker
    - name: Set up Docker
      run: |
        curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
        add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
        apt-get update
        apt-get install -y docker-ce docker-ce-cli containerd.io

    # Install NVIDIA Container Toolkit
    - name: Install NVIDIA Container Toolkit
      run: |
        distribution=$(. /etc/os-release; echo $ID$VERSION_ID | sed -e 's/\.//g')
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
        curl -fsSL https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
          sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
          tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
        apt-get update
        apt-get install -y nvidia-container-toolkit
        nvidia-ctk runtime configure --runtime=docker
        service docker restart

    - name: Log in to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Pull VLLM Docker Image
      run: docker pull vllm/vllm-openai:latest

    - name: Run VLLM Container
      run: |
        docker run -d --runtime nvidia --gpus all \
          --name my_vllm_container \
          -v ~/.cache/huggingface:/root/.cache/huggingface \
          --env "HUGGING_FACE_HUB_TOKEN=${{ secrets.HUGGING_FACE_HUB_TOKEN }}" \
          -p 8000:8000 \
          --ipc=host \
          vllm/vllm-openai:latest \
          --model meta-llama/Llama-3.2-1B

    - name: Test Server Health
      run: |
        sleep 30  # Increased wait time for server startup
        curl -X POST "http://localhost:8000/v1/completions" \
          -H "Content-Type: application/json" \
          --data '{
            "model": "meta-llama/Llama-3.2-1B",
            "prompt": "Health Check:",
            "max_tokens": 5,
            "temperature": 0.1
          }'

    - name: Cleanup
      if: always()
      run: docker container prune -f
